{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re #RegEx\n# NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\n# BOW\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.metrics import accuracy_score\n# pkl\nimport pickle\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nprint(data.shape)\ndata.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['review'][0] # checking how one review is written","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**so a review contains - html tags , special charcaters and mixed case words.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **We need to clean the reviews **\n# Steps involved are:\n* **Remove HTML tags**\n* **Remove special characters**\n* **Convert everything to lowercase**\n* **Remove stopwords**\n* **Stemming**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Removing html tags using regex","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    cleaned = re.compile(r'<.*?>')\n    return re.sub(cleaned,'',text)\n\ndata.review = data.review.apply(clean)\ndata.review[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** We can html tags is removed**\n\n# 2. Removing special charcters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_special(text):\n    rem = ''\n    for i in text:\n        if i.isalnum():\n            rem = rem + i\n        else:\n            rem = rem + ' '\n    return rem\n\ndata.review = data.review.apply(is_special)\ndata.review[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SO special words are also removed**\n\n# 3. Converting the mixed case review to lower case","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_lower(text):\n    return text.lower()\n\ndata.review = data.review.apply(to_lower)\ndata.review[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Now that we have removed special characters, html tags and have turned the mixed case review to lower case**\n \n# 4.Removing stopwords using NLP packages ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rem_stopwords(text):\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(text)\n    return [w for w in words if w not in stop_words]\n\ndata.review = data.review.apply(rem_stopwords)\ndata.review[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **What are these stopwords in English**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Stemming words\n\n**What is stemming?**\n\n**Turning all the words convert back to its original words**\n\n**Example - 'play','playing,'played' all these 3 words are getting converted to its very basic word - 'play'.This process is called stemming.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def stem_txt(text):\n    ss = SnowballStemmer('english')\n    return \" \".join([ss.stem(w) for w in text])\n\ndata.review = data.review.apply(stem_txt)\ndata.review[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We are done with text pre-processing. My reviews are now cleaned**\n\n# Now we will be creating our model.\n\n# 1. Creating bag of words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(data.iloc[:,0].values)\ny = np.array(data.sentiment.values)\ncv = CountVectorizer(max_features = 1000)\nX = cv.fit_transform(data.review).toarray()\nprint(\"X.shape = \",X.shape)\nprint(\"y.shape = \",y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Train test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainx,testx,trainy,testy = train_test_split(X,y,test_size=0.2,random_state=9)\nprint(\"Train shapes : X = {}, y = {}\".format(trainx.shape,trainy.shape))\nprint(\"Test shapes : X = {}, y = {}\".format(testx.shape,testy.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Fitting my data in several algorithms and training them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb,mnb,bnb = GaussianNB(),MultinomialNB(alpha=1.0,fit_prior=True),BernoulliNB(alpha=1.0,fit_prior=True)\ngnb.fit(trainx,trainy)\nmnb.fit(trainx,trainy)\nbnb.fit(trainx,trainy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Prediction,testing stage and also checking the accuracy of the matrix (in-order to choose the best model for this kind of analysis)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ypg = gnb.predict(testx)\nypm = mnb.predict(testx)\nypb = bnb.predict(testx)\n\nprint(\"Gaussian = \",accuracy_score(testy,ypg))\nprint(\"Multinomial = \",accuracy_score(testy,ypm))\nprint(\"Bernoulli = \",accuracy_score(testy,ypb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bernoulli Naive bayes is giving me a better accuracy and hence it is the best model for sentiment analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"revs = \"When I was looking through IMDb's Top 250 movie list and saw a movie called 3 Idiots,I was surprised. Why would a movie with such a bad title be voted so highly? I went in thinking this would be India's version of Animal House or American Pie. But after watching the film, I was in love! It is so much more than the usual college story of young guys getting drunk, flunking classes, and getting back at their superiors. It is an extremely well-made film about doing what you love and facing your fears. The acting is incredible by the ensemble cast. The script is funny and poignant at the same time. Even the scenery is breathtaking. Although the length of the film is pretty long and has some quirky musical numbers, 3 Idiots is a delight. It is worth the watch!\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = clean(revs)\nf2 = is_special(f1)\nf3 = to_lower(f2)\nf4 = rem_stopwords(f3)\nf5 = stem_txt(f4)\n\nbow,words = [],word_tokenize(f5)\nfor word in words:\n    bow.append(words.count(word))\n#np.array(bow).reshape(1,3000)\n#bow.shape\nword_dict = cv.vocabulary_\npickle.dump(word_dict,open('bow1.pkl','wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = []\nfor i in word_dict:\n    inp.append(f5.count(i[0]))\ny_pred = bnb.predict(np.array(inp).reshape(1,1000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(bnb,open('model1.pkl','wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}